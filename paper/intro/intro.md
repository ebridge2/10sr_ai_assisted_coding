# Introduction

The integration of artificial intelligence into scientific computing represents one of the most significant shifts in research methodology since the advent of personal computers. Large language models (LLMs) trained on a vast corpora of code can now generate syntactically correct, functionally appropriate programs from natural language descriptions, a capability that was inconceivable just a few years ago \cite{chen2021evaluating}. Tools like GitHub Copilot, ChatGPT, and Claude have democratized access to sophisticated programming assistance, enabling researchers with limited coding experience to implement complex analyses and build robust scientific software \cite{peng2023impact}. Agentic coding tools like Claude Code and Cursor have further enabled entire coding workflows by enabling the use of tools outside the language model.

AI-assisted coding tools have demonstrated measurable productivity gains in some controlled studies, with benefits spanning development speed, code quality, and maintainability \cite{peng2023impact}. Yet the evidence for these benefits remains contested and context-dependent. While some enterprise studies report productivity increases \cite{kalliamvakou2024measuring}, a 2025 randomized controlled trial with experienced developers found that AI tools actually slowed completion times, despite developers believing they were working faster \cite{becker2025measuring}. These contradictory findings suggest that productivity effects are far from well-understood, and may vary based on developer experience, task complexity, and codebase characteristics \cite{weisz2025examining}, raising important questions about when and how AI assistance truly helps versus hinders development work. These concerns become particularly acute in scientific computing, where code is not merely a means to an end but often embodies scientific reasoning, methodological decisions, and domain expertise. The validity, reproducibility, and interpretability of scientific software directly impact research integrity and the reliability of scientific findings.

The implications for scientific computing are profound. Programming involves complex problem decomposition, algorithmic thinking, and domain-specific reasoning - cognitive skills that may atrophy with excessive AI dependence. Furthermore, scientific code often requires deep understanding of mathematical models, statistical methods, and domain-specific conventions that cannot be adequately captured by AI tools trained on general programming corpora.

The rules presented in this paper emerge from our collective experience using AI-assisted coding tools, and highlight both the substantial promise and documented risks of AI-assisted coding in scientific contexts. We hope they provide a framework for harnessing AI's transformative potential while preserving the methodological rigor and domain expertise essential for high-quality scientific computing. These guidelines emphasize the importance of maintaining human agency in the coding process, establishing robust testing and validation procedures, and strategically managing the interaction between human expertise and AI assistance.
