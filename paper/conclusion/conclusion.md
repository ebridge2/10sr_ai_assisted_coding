# Conclusion

## Ethical Considerations and Responsibility

The use of AI-assisted coding raises fundamental questions about scientific accountability. When code that generates published results is partly AI-generated, who bears responsibility for errors, methodological flaws, or irreproducible outcomes? The answer must be unequivocal: the scientist. AI tools are instruments, and like any instrument in science, the researcher using them remains fully accountable for validating their outputs and ensuring methodological soundness. This responsibility cannot be delegated to the AI, regardless of how sophisticated the tool or how confident its outputs appear. Researchers must ensure their AI-assisted code is reproducible, well-documented, and scientifically appropriate. When AI generates code that implements a statistical method or analytical pipeline, the researcher must understand that implementation well enough to defend its appropriateness, explain its limitations, and troubleshoot unexpected results. "AI wrote it" is not a valid defense for flawed methodology or incorrect results. Transparency about AI usage in methods sections, while important, does not diminish this responsibility.

Beyond individual accountability, broader ethical concerns demand serious consideration. The environmental costs of training and running large language models are substantial and measurable {cite}`faiz2024llmcarbon`, {cite}`luccioni2024environmental`. These systems consume enormous amounts of energy and computational resources, raising questions about the sustainability of widespread AI adoption. Further, intellectual property questions surrounding AI training on open-source code and the ownership of AI-generated code remain legally and ethically unsettled {cite}`nordemann2022copyright`, {cite}`sag2025copyright`, {cite}`blaszczyk2024ai`, {cite}`kretschmer2024copyright`. Courts have yet to definitively rule on whether training on copyrighted code constitutes fair use, whether AI-generated code can be copyrighted, and who owns the rights to such code when models have been trained on proprietary or licensed material. These are fundamental ethical and legal challenges that the scientific community must grapple with as AI tools become embedded in research infrastructure. While these complex issues merit dedicated treatment beyond our scope here, researchers should recognize that using AI coding tools involves participating in systems with significant unresolved ethical dimensions.

## Guardrails for Autonomous Agents

Autonomous coding agents can make extensive changes across a codebase with minimal human intervention, dramatically accelerating development but introducing risks if not properly constrained. The primary danger lies in granting agents too much control without appropriate safeguards. An agent given broad permissions might break existing functionality, introduce security vulnerabilities, or violate architectural principles while reporting success.

We recommend several guardrails. First, use containerized or sandboxed environments for agent-driven development, isolating agent operations from production systems \cite{TODO: cite further reading book}. Second, commit working code before allowing agent changes, enabling easy rollback. Third, learn how to properly configure agents with explicit constraints about what they can modify and what actions require human approval. Fourth, maintain active monitoring rather than allowing unsupervised operation, as discussed in Rule 8. For individual projects, consider project-specific containers where each agent operates in an isolated environment with restricted file access. As autonomous agents become more capable, developing clear and safe practices for constraining and monitoring their behavior will become increasingly important for maintaining scientific rigor and system safety.

## Limitations and Future Directions

We acknowledge that we are operating in a rapidly evolving technological landscape. For reference, GPT-3 (2020) had a context window of 2,048 tokens {cite}`brown2020language`, GPT-4 (2023) expanded this to variants with tens of thousands of tokens {cite}`openai2023gpt4`, and current state-of-the-art models like Claude Gemini 2.5 ProSonnet 4.5 (2025) can operate with context windows of millions {cite}`google2024gemini2`. Despite this rapid evolution, we have intentionally focused on principles and practices that remain relevant across different AI capabilities. We believe our proposed rules emphasize fundamental skills (domain knowledge, problem decomposition, critical review) and strategies (context management, test-driven development, incremental refinement) that apply regardless of specific tools, and thus far have proven useful throughout the evolution of AI models to-date. We deliberately avoided prescriptive recommendations and strategies tied to specific models, as these would quickly become outdated. Future advances may change which practices prove most valuable, but we believe these rules provide a useful framework for current practice which will remain adaptable as technology matures.

## Further Reading

The rules presented in this paper provide a framework for using AI tools effectively in scientific computing, but they build upon foundational concepts in software development, reproducibility, and design. The following resources introduce key concepts and practices that support the development of programming skills necessary for effective AI-assisted coding, and may help readers deepen their understanding of principles underlying the rules.

1. LeVeque, Randall J., et al. "Reproducible Research for Scientific Computing: Tools and Strategies." Computing in Science & Engineering, vol. 14, no. 4, 2012, pp. 13-17, doi:10.1109/MCSE.2012.38. This article establishes best practices for reproducible scientific computing that inform our emphasis on documentation, testing, and validation throughout the rules {cite}`leveque2012reproducible`.
2. Ousterhout, John. A Philosophy of Software Design. 2nd ed., Yaknyam Press, 2021. This book articulates core principles of software design including abstraction and modularity that inform our guidance on how to think about programmatic solutions and what to specify as useful context for scientific problems in Rules 4 and 5, as well as how to think through incremental refinement stages for Rule 10 {cite}`ousterhout2021philosophy`.
3. Poldrack, Russell A. Better Code, Better Science. https://poldrack.github.io/BetterCodeBetterScience/frontmatter.html. Accessed Sept 10, 2025. This comprehensive guide introduces AI tools in scientific workflows and provides practical guidance that complements the principles outlined in our rules {cite}`poldrack2024better`.
4. Felleisen, Matthias, et al. How to Design Programs: An Introduction to Programming and Computing. 2nd ed., MIT Press, 2018, https://htdp.org. This text emphasizes systematic problem decomposition and design principles that underpin the distinction between programming and coding discussed in Rule 2 {cite}`felleisen2018design`.
5. Beck, Kent. Test-Driven Development: By Example. Addison-Wesley, 2003. This book provides the foundational methodology for test-driven development discussed in Rule 6, demonstrating how writing tests before implementation can help you to develop more robust and maintainable code {cite}`beck2003test`.
6. Wiebels, Kristina, and David Moreau. "Leveraging Containers for Reproducible Psychological Research." Advances in Methods and Practices in Psychological Science, vol. 4, no. 2, 2021, doi:10.1177/25152459211017853. This paper demonstrates how containerization supports reproducible research, directly relevant to the guardrails for autonomous agents discussed in our Discussion section {cite}`wiebels2021leveraging`.

## Acknowledgements

An initial framework of 20 rules was developed by EWB and RAP (10 rules each). These rules were streamlined into 10 rules with assistance from Claude (Anthropic) {cite}`anthropic2025claude45`, which were then authored and iteratively refined by the research team into the content, examples, and recommendations presented herein. This work was supported by a grant from the Sloan Foundation to RAP (G-2025-25270).

## References

```{bibliography}
:filter: docname in docnames
```